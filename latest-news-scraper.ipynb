{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc136347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T15:14:36.778724Z",
     "iopub.status.busy": "2025-05-21T15:14:36.778183Z",
     "iopub.status.idle": "2025-05-21T15:15:00.727005Z",
     "shell.execute_reply": "2025-05-21T15:15:00.725559Z"
    },
    "papermill": {
     "duration": 23.954948,
     "end_time": "2025-05-21T15:15:00.728886",
     "exception": false,
     "start_time": "2025-05-21T15:14:36.773938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\r\n",
      "Collecting googlenews\r\n",
      "  Downloading GoogleNews-1.6.15-py3-none-any.whl.metadata (4.5 kB)\r\n",
      "Collecting newspaper3k\r\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\r\n",
      "Collecting lxml_html_clean\r\n",
      "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from googlenews) (4.13.3)\r\n",
      "Collecting dateparser (from googlenews)\r\n",
      "  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\r\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from googlenews) (2.9.0.post0)\r\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\r\n",
      "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\r\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\r\n",
      "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.1)\r\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\r\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\r\n",
      "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\r\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\r\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\r\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->googlenews) (2.6)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->googlenews) (4.13.2)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\r\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\r\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\r\n",
      "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\r\n",
      "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser->googlenews) (2025.2)\r\n",
      "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser->googlenews) (5.3.1)\r\n",
      "Downloading GoogleNews-1.6.15-py3-none-any.whl (8.8 kB)\r\n",
      "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\r\n",
      "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\r\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading dateparser-1.2.1-py3-none-any.whl (295 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\r\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\r\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=8fb292ed74c784ad0057c52bb1fad1d387ca2e62944308a24ecabea3ac929247\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\r\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=5cb476903ef4fa25af8fc32c9fe4e028a13165f46daa78eac856e3fc23cdf5aa\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\r\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=3a55290aeeb4a5741593243855235789994c4c00624ae33744d7e863e6391702\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\r\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=92135e944b1476d95822346cb00294f539bcd851f6098e7667870d667ff5c6a8\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\r\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\r\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, lxml_html_clean, feedparser, cssselect, requests-file, feedfinder2, dateparser, tldextract, googlenews, newspaper3k\r\n",
      "Successfully installed cssselect-1.3.0 dateparser-1.2.1 feedfinder2-0.0.4 feedparser-6.0.11 googlenews-1.6.15 jieba3k-0.35.1 lxml_html_clean-0.4.2 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.3.1)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk googlenews newspaper3k requests lxml_html_clean \n",
    "%pip install lxml beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e09a968",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-21T15:15:00.740680Z",
     "iopub.status.busy": "2025-05-21T15:15:00.740337Z",
     "iopub.status.idle": "2025-05-21T15:15:04.894171Z",
     "shell.execute_reply": "2025-05-21T15:15:04.892518Z"
    },
    "papermill": {
     "duration": 4.161446,
     "end_time": "2025-05-21T15:15:04.896017",
     "exception": false,
     "start_time": "2025-05-21T15:15:00.734571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from GoogleNews import GoogleNews\n",
    "from newspaper import Article, Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b0779",
   "metadata": {
    "papermill": {
     "duration": 0.00534,
     "end_time": "2025-05-21T15:15:04.906460",
     "exception": false,
     "start_time": "2025-05-21T15:15:04.901120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensure punkt tokenizer is available\n",
    "# For production use, consider including punkt with dependencies instead of downloading at runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56328bc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T15:15:04.921638Z",
     "iopub.status.busy": "2025-05-21T15:15:04.921172Z",
     "iopub.status.idle": "2025-05-21T15:15:04.937754Z",
     "shell.execute_reply": "2025-05-21T15:15:04.935878Z"
    },
    "papermill": {
     "duration": 0.026113,
     "end_time": "2025-05-21T15:15:04.940065",
     "exception": false,
     "start_time": "2025-05-21T15:15:04.913952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    raise LookupError(\"The NLTK 'punkt' tokenizer is missing. Please install it using nltk.download('punkt') during setup.\")\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(name)s: %(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger('NewsScraper')\n",
    "\n",
    "# Configure HTTP session with retries\n",
    "USER_AGENT = ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) '\n",
    "              'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "              'Chrome/50.0.2661.102 Safari/537.36')\n",
    "session = requests.Session()\n",
    "session.headers.update({'User-Agent': USER_AGENT})\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=0.5,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# Newspaper config\n",
    "np_config = Config()\n",
    "np_config.browser_user_agent = USER_AGENT\n",
    "np_config.request_timeout = 10\n",
    "\n",
    "# Constants\n",
    "DEFAULT_MAX_PAGES = 5\n",
    "CSV_MAX_SIZE = 5 * 1024 * 1024\n",
    "BATCH_SIZE = 10\n",
    "FIELDNAMES = ['Title','Slug','Excerpt','Content','Article','Summary','Image Featured','Format','Date','Categories','Tags']\n",
    "CATEGORY_KEYWORDS = {\n",
    "    'Entertainment':['movie','music','show'],\n",
    "    'Sports':['sport','game','match'],\n",
    "    'Business and Finance':['market','finance','business'],\n",
    "    'Health and Foods':['health','food','wellness'],\n",
    "    'Life Style':['lifestyle','travel'],\n",
    "    'Politics':['politics','election','government'],\n",
    "    'World News':['world','international'],\n",
    "    'Technology':['tech','software','internet'],\n",
    "    'Travel-  Life Style':['travel','vacation'],\n",
    "    'Flower':['flower','floral'],\n",
    "    'Fashion':['fashion','runway'],\n",
    "    'Uncategorized':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6d5ceb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T15:15:04.952817Z",
     "iopub.status.busy": "2025-05-21T15:15:04.952430Z",
     "iopub.status.idle": "2025-05-21T15:15:04.973135Z",
     "shell.execute_reply": "2025-05-21T15:15:04.971774Z"
    },
    "papermill": {
     "duration": 0.029244,
     "end_time": "2025-05-21T15:15:04.975053",
     "exception": false,
     "start_time": "2025-05-21T15:15:04.945809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def slugify(text: str) -> str:\n",
    "    slug = re.sub(r\"-+\", '-', re.sub(r\"[^a-z0-9]+\", '-', text.lower())).strip('-')\n",
    "    logger.debug(f\"Slugified '{text}' to '{slug}'\")\n",
    "    return slug\n",
    "\n",
    "def assign_category(title: str, tags: list) -> str:\n",
    "    combined = (title + ' ' + ' '.join(tags)).lower()\n",
    "    for cat, kws in CATEGORY_KEYWORDS.items():\n",
    "        if any(kw in combined for kw in kws):\n",
    "            logger.debug(f\"Assigned category '{cat}' for title '{title}'\")\n",
    "            return cat\n",
    "    logger.debug(f\"Assigned default category 'Uncategorized' for title '{title}'\")\n",
    "    return 'Uncategorized'\n",
    "\n",
    "class RotatingCSVWriter:\n",
    "    def __init__(self, base_name: str):\n",
    "        self.base = base_name\n",
    "        self.index = 1\n",
    "        self.buffer = []\n",
    "        self._open()\n",
    "    def _open(self):\n",
    "        if hasattr(self,'file'):\n",
    "            logger.debug(f\"Closing file {self.file.name}\")\n",
    "            self._flush()\n",
    "            self.file.close()\n",
    "        fname = f\"{self.base}_{self.index}.csv\"\n",
    "        logger.debug(f\"Opening CSV file: {fname}\")\n",
    "        self.file = open(fname,'w',encoding='utf-8',newline='')\n",
    "        self.writer = csv.DictWriter(self.file, fieldnames=FIELDNAMES)\n",
    "        self.writer.writeheader()\n",
    "    def write(self,row:dict):\n",
    "        logger.debug(f\"Buffering row with Title: {row.get('Title')} to {self.file.name}\")\n",
    "        self.buffer.append(row)\n",
    "        if len(self.buffer) >= BATCH_SIZE:\n",
    "            self._flush()\n",
    "    def _flush(self):\n",
    "        logger.debug(f\"Flushing {len(self.buffer)} rows to disk\")\n",
    "        for row in self.buffer:\n",
    "            self.writer.writerow(row)\n",
    "        self.file.flush()\n",
    "        self.buffer = []\n",
    "        size = os.path.getsize(self.file.name)\n",
    "        logger.debug(f\"Current file size: {size} bytes\")\n",
    "        if size >= CSV_MAX_SIZE:\n",
    "            logger.debug(f\"File size exceeded {CSV_MAX_SIZE}, rotating file\")\n",
    "            self.index += 1\n",
    "            self._open()\n",
    "    def close(self):\n",
    "        logger.debug(f\"Closing final file {self.file.name}\")\n",
    "        if self.buffer:\n",
    "            self._flush()\n",
    "        self.file.close()\n",
    "\n",
    "def fetch_articles_for_category(category: str, year: int, pages: int, writer: RotatingCSVWriter):\n",
    "    logger.debug(f\"Fetching category '{category}' for year {year}\")\n",
    "    googlenews = GoogleNews(lang='en', region='US')\n",
    "    query = f\"{category} after:{year}-01-01 before:{year+1}-01-01\"\n",
    "    logger.debug(f\"GoogleNews query: {query}\")\n",
    "    googlenews.search(query)\n",
    "    seen = set()\n",
    "    article_parser = Article('', config=np_config)\n",
    "    for p in range(1,pages+1):\n",
    "        if p>1:\n",
    "            logger.debug(f\"Fetching GoogleNews page {p} for category '{category}'\")\n",
    "            googlenews.getpage(p)\n",
    "        results = googlenews.results()\n",
    "        logger.debug(f\"Retrieved {len(results)} results on page {p}\")\n",
    "        if not results:\n",
    "            break\n",
    "        for res in results:\n",
    "            title = res.get('title','').strip()\n",
    "            logger.debug(f\"Processing result title: {title}\")\n",
    "            if not title or title in seen:\n",
    "                continue\n",
    "            seen.add(title)\n",
    "            link = res.get('link','').split('&')[0]\n",
    "            logger.debug(f\"Using link: {link}\")\n",
    "            excerpt = res.get('desc','').strip()\n",
    "            try:\n",
    "                resp = session.get(link,timeout=10)\n",
    "                resp.raise_for_status()\n",
    "                html = resp.text\n",
    "                logger.debug(f\"Prefetched HTML for {link}\")\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed to prefetch {link}: {e}\")\n",
    "                continue\n",
    "            slug = slugify(title)\n",
    "            tags=[]\n",
    "            date_str = res.get('date','').strip()\n",
    "            content=excerpt\n",
    "            article_text='' ; summary='' ; image=''\n",
    "            try:\n",
    "                article_parser.set_url(link)\n",
    "                article_parser.download(input_html=html)\n",
    "                article_parser.parse()\n",
    "                article_parser.nlp()\n",
    "                article_text = article_parser.text\n",
    "                summary = article_parser.summary\n",
    "                image = article_parser.top_image or ''\n",
    "                tags = article_parser.keywords or []\n",
    "                if article_parser.publish_date:\n",
    "                    date_str = article_parser.publish_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                logger.debug(f\"Parsed article '{title}' with {len(tags)} tags\")\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Article parsing failed for '{title}': {e}\")\n",
    "            cat_assigned = assign_category(title, tags)\n",
    "            row = {\n",
    "                'Title':title, 'Slug':slug, 'Excerpt':excerpt, 'Content':content,\n",
    "                'Article':article_text, 'Summary':summary, 'Image Featured':image,\n",
    "                'Format':'standard', 'Date':date_str, 'Categories':cat_assigned,\n",
    "                'Tags':'|'.join(tags)\n",
    "            }\n",
    "            writer.write(row)\n",
    "        googlenews.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a76de09e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T15:15:04.986240Z",
     "iopub.status.busy": "2025-05-21T15:15:04.985855Z",
     "iopub.status.idle": "2025-05-21T15:38:38.757204Z",
     "shell.execute_reply": "2025-05-21T15:38:38.756260Z"
    },
    "papermill": {
     "duration": 1413.779234,
     "end_time": "2025-05-21T15:38:38.759028",
     "exception": false,
     "start_time": "2025-05-21T15:15:04.979794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n"
     ]
    }
   ],
   "source": [
    "year=2025\n",
    "pages=\"10000\"\n",
    "pages=int(pages) if pages.isdigit() else DEFAULT_MAX_PAGES\n",
    "logger.debug(f\"Starting main with year={year}, pages={pages}\")\n",
    "writer=RotatingCSVWriter(f\"news_{year}\")\n",
    "for cat in CATEGORY_KEYWORDS:\n",
    "    fetch_articles_for_category(cat,year,pages,writer)\n",
    "writer.close()\n",
    "logger.debug(\"All categories processed, exiting main\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1448.238543,
   "end_time": "2025-05-21T15:38:39.687764",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-21T15:14:31.449221",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
